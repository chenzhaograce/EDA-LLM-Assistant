{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2479091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä EDA LLM Assistant Initialized!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# EDA LLM Assistant - Comprehensive Exploratory Data Analysis\n",
    "\n",
    "\"\"\"\n",
    "This notebook provides a complete EDA framework with automated analysis capabilities.\n",
    "It combines traditional EDA techniques with modern automated profiling tools.\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"üìä EDA LLM Assistant Initialized!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ff87bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß EDA Assistant class created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Initial Setup\n",
    "\n",
    "class EDAAssistant:\n",
    "    \"\"\"\n",
    "    A comprehensive EDA assistant class that provides automated analysis capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.numeric_columns = []\n",
    "        self.categorical_columns = []\n",
    "        self.datetime_columns = []\n",
    "        \n",
    "    def load_data(self, file_path, table_name=None, query=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Load data from various file formats including SQLite databases\n",
    "        \n",
    "        Parameters:\n",
    "        file_path: Path to the data file\n",
    "        table_name: For SQLite databases, specify the table name to load\n",
    "        query: For SQLite databases, specify a custom SQL query\n",
    "        **kwargs: Additional parameters for pandas read functions\n",
    "        \"\"\"\n",
    "        file_extension = Path(file_path).suffix.lower()\n",
    "        \n",
    "        try:\n",
    "            if file_extension == '.csv':\n",
    "                self.data = pd.read_csv(file_path, **kwargs)\n",
    "            elif file_extension in ['.xlsx', '.xls']:\n",
    "                self.data = pd.read_excel(file_path, **kwargs)\n",
    "            elif file_extension == '.json':\n",
    "                self.data = pd.read_json(file_path, **kwargs)\n",
    "            elif file_extension == '.parquet':\n",
    "                self.data = pd.read_parquet(file_path, **kwargs)\n",
    "            elif file_extension in ['.db', '.sqlite', '.sqlite3']:\n",
    "                self.data = self._load_sqlite_data(file_path, table_name, query, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
    "                \n",
    "            print(f\"‚úÖ Data loaded successfully!\")\n",
    "            print(f\"Shape: {self.data.shape}\")\n",
    "            self._analyze_column_types()\n",
    "            return self.data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_sqlite_data(self, file_path, table_name=None, query=None, **kwargs):\n",
    "        \"\"\"Load data from SQLite database\"\"\"\n",
    "        import sqlite3\n",
    "        \n",
    "        # Create connection to SQLite database\n",
    "        conn = sqlite3.connect(file_path)\n",
    "        \n",
    "        try:\n",
    "            if query:\n",
    "                # Use custom query\n",
    "                print(f\"üîç Executing custom query...\")\n",
    "                data = pd.read_sql_query(query, conn, **kwargs)\n",
    "            elif table_name:\n",
    "                # Load specific table\n",
    "                print(f\"üìä Loading table: {table_name}\")\n",
    "                data = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn, **kwargs)\n",
    "            else:\n",
    "                # If no table specified, show available tables and load the first one\n",
    "                tables = self._get_sqlite_tables(conn)\n",
    "                if not tables:\n",
    "                    raise ValueError(\"No tables found in the SQLite database\")\n",
    "                \n",
    "                print(f\"üìã Available tables: {tables}\")\n",
    "                table_name = tables[0]\n",
    "                print(f\"üìä Loading first table: {table_name}\")\n",
    "                data = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn, **kwargs)\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def _get_sqlite_tables(self, conn):\n",
    "        \"\"\"Get list of tables in SQLite database\"\"\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "        return tables\n",
    "    \n",
    "    def explore_sqlite_database(self, file_path):\n",
    "        \"\"\"Explore SQLite database structure\"\"\"\n",
    "        import sqlite3\n",
    "        \n",
    "        conn = sqlite3.connect(file_path)\n",
    "        \n",
    "        try:\n",
    "            print(\"üóÑÔ∏è  SQLITE DATABASE EXPLORATION\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # Get all tables\n",
    "            tables = self._get_sqlite_tables(conn)\n",
    "            print(f\"üìã Tables found: {len(tables)}\")\n",
    "            \n",
    "            for table in tables:\n",
    "                print(f\"\\nüìä TABLE: {table}\")\n",
    "                \n",
    "                # Get table info\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "                columns_info = cursor.fetchall()\n",
    "                \n",
    "                print(\"   Columns:\")\n",
    "                for col_info in columns_info:\n",
    "                    col_name, col_type = col_info[1], col_info[2]\n",
    "                    print(f\"     - {col_name} ({col_type})\")\n",
    "                \n",
    "                # Get row count\n",
    "                cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "                row_count = cursor.fetchone()[0]\n",
    "                print(f\"   Rows: {row_count:,}\")\n",
    "                \n",
    "                # Show sample data\n",
    "                cursor.execute(f\"SELECT * FROM {table} LIMIT 3\")\n",
    "                sample_data = cursor.fetchall()\n",
    "                if sample_data:\n",
    "                    print(\"   Sample data (first 3 rows):\")\n",
    "                    column_names = [col_info[1] for col_info in columns_info]\n",
    "                    sample_df = pd.DataFrame(sample_data, columns=column_names)\n",
    "                    print(sample_df.to_string(index=False))\n",
    "            \n",
    "            return tables\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error exploring database: {str(e)}\")\n",
    "            return []\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def _analyze_column_types(self):\n",
    "        \"\"\"Analyze and categorize column types\"\"\"\n",
    "        self.numeric_columns = self.data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.categorical_columns = self.data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        self.datetime_columns = self.data.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "        \n",
    "        print(f\"üìä Column Analysis:\")\n",
    "        print(f\"   Numeric columns: {len(self.numeric_columns)}\")\n",
    "        print(f\"   Categorical columns: {len(self.categorical_columns)}\")\n",
    "        print(f\"   DateTime columns: {len(self.datetime_columns)}\")\n",
    "\n",
    "# Initialize EDA Assistant\n",
    "eda = EDAAssistant()\n",
    "\n",
    "# Example data loading (uncomment and modify path as needed)\n",
    "# eda.load_data('your_dataset.csv')\n",
    "\n",
    "print(\"üîß EDA Assistant class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite Database Support and Examples\n",
    "\n",
    "def create_sample_sqlite_database(db_path=\"sample_data.db\"):\n",
    "    \"\"\"Create a sample SQLite database for demonstration\"\"\"\n",
    "    import sqlite3\n",
    "    \n",
    "    # Generate sample data\n",
    "    sample_data = generate_sample_data(1000)\n",
    "    \n",
    "    # Create SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    try:\n",
    "        # Create main customers table\n",
    "        customers_data = sample_data[['age', 'income', 'score', 'category', 'city', 'is_premium', 'rating']].copy()\n",
    "        customers_data.to_sql('customers', conn, if_exists='replace', index_label='customer_id')\n",
    "        \n",
    "        # Create transactions table (derived from customers)\n",
    "        np.random.seed(42)\n",
    "        transactions_data = []\n",
    "        for customer_id in range(len(customers_data)):\n",
    "            # Generate 1-5 transactions per customer\n",
    "            num_transactions = np.random.randint(1, 6)\n",
    "            for _ in range(num_transactions):\n",
    "                transaction = {\n",
    "                    'customer_id': customer_id,\n",
    "                    'transaction_date': sample_data.iloc[customer_id]['registration_date'] + pd.Timedelta(days=np.random.randint(0, 365)),\n",
    "                    'amount': np.random.exponential(100),\n",
    "                    'product_category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports'])\n",
    "                }\n",
    "                transactions_data.append(transaction)\n",
    "        \n",
    "        transactions_df = pd.DataFrame(transactions_data)\n",
    "        transactions_df.to_sql('transactions', conn, if_exists='replace', index_label='transaction_id')\n",
    "        \n",
    "        # Create products table\n",
    "        products_data = pd.DataFrame({\n",
    "            'product_name': ['Laptop', 'Smartphone', 'Headphones', 'T-Shirt', 'Jeans', \n",
    "                           'Novel', 'Cookbook', 'Chair', 'Table', 'Basketball'],\n",
    "            'category': ['Electronics', 'Electronics', 'Electronics', 'Clothing', 'Clothing',\n",
    "                        'Books', 'Books', 'Home', 'Home', 'Sports'],\n",
    "            'price': [999.99, 699.99, 199.99, 29.99, 79.99, \n",
    "                     14.99, 24.99, 149.99, 299.99, 39.99],\n",
    "            'in_stock': [True, True, False, True, True, True, True, False, True, True]\n",
    "        })\n",
    "        products_data.to_sql('products', conn, if_exists='replace', index_label='product_id')\n",
    "        \n",
    "        print(f\"‚úÖ Sample SQLite database created: {db_path}\")\n",
    "        print(\"üìã Tables created:\")\n",
    "        print(\"   ‚Ä¢ customers (1000 rows) - Customer demographics and ratings\")\n",
    "        print(\"   ‚Ä¢ transactions (~3000 rows) - Purchase transactions\")\n",
    "        print(\"   ‚Ä¢ products (10 rows) - Product catalog\")\n",
    "        \n",
    "        return db_path\n",
    "        \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def demonstrate_sqlite_loading():\n",
    "    \"\"\"Demonstrate different ways to load SQLite data\"\"\"\n",
    "    print(\"üóÑÔ∏è  SQLITE DATA LOADING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create sample database\n",
    "    db_path = create_sample_sqlite_database()\n",
    "    \n",
    "    print(\"\\nüìñ LOADING METHODS:\")\n",
    "    print(\"1. Explore database structure:\")\n",
    "    print(\"   eda.explore_sqlite_database('sample_data.db')\")\n",
    "    \n",
    "    print(\"\\n2. Load entire table:\")\n",
    "    print(\"   eda.load_data('sample_data.db', table_name='customers')\")\n",
    "    \n",
    "    print(\"\\n3. Load with custom SQL query:\")\n",
    "    print(\"   query = 'SELECT * FROM customers WHERE age > 30 AND rating >= 4'\")\n",
    "    print(\"   eda.load_data('sample_data.db', query=query)\")\n",
    "    \n",
    "    print(\"\\n4. Load with JOIN across tables:\")\n",
    "    print(\"   query = '''\")\n",
    "    print(\"   SELECT c.age, c.income, c.city, t.amount, t.product_category\")\n",
    "    print(\"   FROM customers c\")\n",
    "    print(\"   JOIN transactions t ON c.customer_id = t.customer_id\")\n",
    "    print(\"   WHERE c.is_premium = 1\")\n",
    "    print(\"   '''\")\n",
    "    print(\"   eda.load_data('sample_data.db', query=query)\")\n",
    "    \n",
    "    print(\"\\n5. Auto-load first table (if no table specified):\")\n",
    "    print(\"   eda.load_data('sample_data.db')\")\n",
    "    \n",
    "    return db_path\n",
    "\n",
    "# Add SQLite methods to EDA Assistant\n",
    "def add_sqlite_methods():\n",
    "    EDAAssistant.explore_sqlite_database = lambda self, file_path: self.explore_sqlite_database(file_path)\n",
    "\n",
    "add_sqlite_methods()\n",
    "\n",
    "print(\"‚úÖ SQLite support added to EDA Assistant!\")\n",
    "print(\"\\nüí° SQLite Features:\")\n",
    "print(\"‚Ä¢ Automatic table detection and listing\")\n",
    "print(\"‚Ä¢ Support for custom SQL queries\")\n",
    "print(\"‚Ä¢ Database structure exploration\")\n",
    "print(\"‚Ä¢ JOIN operations across multiple tables\")\n",
    "print(\"‚Ä¢ Supports .db, .sqlite, .sqlite3 file extensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ SQLite DEMONSTRATION: Run this cell to see SQLite support in action!\n",
    "\n",
    "print(\"üóÑÔ∏è  DEMONSTRATING SQLITE DATABASE FUNCTIONALITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create and demonstrate SQLite loading\n",
    "db_path = demonstrate_sqlite_loading()\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \" LIVE DEMO \" + \"=\"*30)\n",
    "\n",
    "# Create a new EDA instance for SQLite demo\n",
    "eda_sqlite = EDAAssistant()\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ EXPLORING DATABASE STRUCTURE:\")\n",
    "tables = eda_sqlite.explore_sqlite_database(db_path)\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ LOADING CUSTOMERS TABLE:\")\n",
    "eda_sqlite.load_data(db_path, table_name='customers')\n",
    "print(f\"Loaded data shape: {eda_sqlite.data.shape}\")\n",
    "print(\"First 3 rows:\")\n",
    "print(eda_sqlite.data.head(3))\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ LOADING WITH CUSTOM QUERY - High-value customers:\")\n",
    "query = \"\"\"\n",
    "SELECT customer_id, age, income, score, rating, city\n",
    "FROM customers \n",
    "WHERE income > 50000 AND rating >= 4\n",
    "ORDER BY income DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "eda_sqlite.load_data(db_path, query=query)\n",
    "print(f\"High-value customers data shape: {eda_sqlite.data.shape}\")\n",
    "print(eda_sqlite.data)\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ COMPLEX JOIN QUERY - Customer transactions analysis:\")\n",
    "complex_query = \"\"\"\n",
    "SELECT \n",
    "    c.age,\n",
    "    c.income,\n",
    "    c.city,\n",
    "    c.rating,\n",
    "    COUNT(t.transaction_id) as transaction_count,\n",
    "    AVG(t.amount) as avg_transaction_amount,\n",
    "    SUM(t.amount) as total_spent\n",
    "FROM customers c\n",
    "LEFT JOIN transactions t ON c.customer_id = t.customer_id\n",
    "GROUP BY c.customer_id, c.age, c.income, c.city, c.rating\n",
    "HAVING transaction_count > 2\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 15\n",
    "\"\"\"\n",
    "\n",
    "print(\"Loading customer transaction analysis...\")\n",
    "eda_sqlite.load_data(db_path, query=complex_query)\n",
    "print(f\"Analysis data shape: {eda_sqlite.data.shape}\")\n",
    "print(eda_sqlite.data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ SQLite demonstration completed!\")\n",
    "print(\"üéØ You can now run EDA analysis on the SQLite data:\")\n",
    "print(\"   eda_sqlite.complete_eda_analysis()\")\n",
    "print(\"   eda_sqlite.correlation_analysis()\")\n",
    "print(\"   eda_sqlite.plot_numeric_distributions()\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013eaa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Exploration Methods\n",
    "\n",
    "def basic_info(data):\n",
    "    \"\"\"Display basic information about the dataset\"\"\"\n",
    "    print(\"üìã DATASET OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(\"\\nüîç DATA TYPES:\")\n",
    "    print(data.dtypes)\n",
    "    \n",
    "    print(\"\\nüìä MISSING VALUES:\")\n",
    "    missing = data.isnull().sum()\n",
    "    missing_percent = (missing / len(data)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing.index,\n",
    "        'Missing Count': missing.values,\n",
    "        'Missing %': missing_percent.values\n",
    "    }).sort_values('Missing %', ascending=False)\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    print(\"\\nüéØ UNIQUE VALUES:\")\n",
    "    unique_counts = data.nunique().sort_values(ascending=False)\n",
    "    print(unique_counts)\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "def statistical_summary(data, numeric_columns):\n",
    "    \"\"\"Generate statistical summary for numeric columns\"\"\"\n",
    "    print(\"\\nüìà STATISTICAL SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if numeric_columns:\n",
    "        stats = data[numeric_columns].describe()\n",
    "        print(stats)\n",
    "        \n",
    "        # Additional statistics\n",
    "        print(\"\\nüìä ADDITIONAL STATISTICS:\")\n",
    "        additional_stats = pd.DataFrame({\n",
    "            'Skewness': data[numeric_columns].skew(),\n",
    "            'Kurtosis': data[numeric_columns].kurtosis(),\n",
    "            'Variance': data[numeric_columns].var()\n",
    "        })\n",
    "        print(additional_stats)\n",
    "    else:\n",
    "        print(\"No numeric columns found for statistical analysis.\")\n",
    "\n",
    "# Add methods to EDA Assistant class\n",
    "def add_basic_methods():\n",
    "    EDAAssistant.basic_info = lambda self: basic_info(self.data)\n",
    "    EDAAssistant.statistical_summary = lambda self: statistical_summary(self.data, self.numeric_columns)\n",
    "\n",
    "add_basic_methods()\n",
    "print(\"‚úÖ Basic exploration methods added to EDA Assistant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1469571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization Functions\n",
    "\n",
    "def plot_missing_values(data):\n",
    "    \"\"\"Visualize missing values pattern\"\"\"\n",
    "    missing = data.isnull().sum()\n",
    "    missing = missing[missing > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if len(missing) > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Missing values bar plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        missing.plot(kind='bar', color='coral')\n",
    "        plt.title('Missing Values Count by Column')\n",
    "        plt.xlabel('Columns')\n",
    "        plt.ylabel('Missing Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Missing values heatmap\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.heatmap(data[missing.index].isnull(), cbar=True, cmap='viridis')\n",
    "        plt.title('Missing Values Heatmap')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"üéâ No missing values found in the dataset!\")\n",
    "\n",
    "def plot_numeric_distributions(data, numeric_columns, max_cols=3):\n",
    "    \"\"\"Plot distributions of numeric columns\"\"\"\n",
    "    if not numeric_columns:\n",
    "        print(\"No numeric columns to plot.\")\n",
    "        return\n",
    "    \n",
    "    n_cols = min(max_cols, len(numeric_columns))\n",
    "    n_rows = (len(numeric_columns) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(numeric_columns):\n",
    "        if i < len(axes):\n",
    "            # Histogram with KDE\n",
    "            data[col].hist(bins=30, alpha=0.7, ax=axes[i], density=True)\n",
    "            data[col].plot(kind='kde', ax=axes[i], color='red')\n",
    "            axes[i].set_title(f'Distribution of {col}')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Density')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical_distributions(data, categorical_columns, max_cols=2, top_n=10):\n",
    "    \"\"\"Plot distributions of categorical columns\"\"\"\n",
    "    if not categorical_columns:\n",
    "        print(\"No categorical columns to plot.\")\n",
    "        return\n",
    "    \n",
    "    n_cols = min(max_cols, len(categorical_columns))\n",
    "    n_rows = (len(categorical_columns) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(8*n_cols, 4*n_rows))\n",
    "    if len(categorical_columns) == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if n_rows > 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(categorical_columns):\n",
    "        if i < len(axes):\n",
    "            # Get top N categories\n",
    "            value_counts = data[col].value_counts().head(top_n)\n",
    "            \n",
    "            # Bar plot\n",
    "            value_counts.plot(kind='bar', ax=axes[i], color='skyblue')\n",
    "            axes[i].set_title(f'Top {min(top_n, len(value_counts))} values in {col}')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Count')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(categorical_columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Add visualization methods to EDA Assistant\n",
    "def add_visualization_methods():\n",
    "    EDAAssistant.plot_missing_values = lambda self: plot_missing_values(self.data)\n",
    "    EDAAssistant.plot_numeric_distributions = lambda self, max_cols=3: plot_numeric_distributions(self.data, self.numeric_columns, max_cols)\n",
    "    EDAAssistant.plot_categorical_distributions = lambda self, max_cols=2, top_n=10: plot_categorical_distributions(self.data, self.categorical_columns, max_cols, top_n)\n",
    "\n",
    "add_visualization_methods()\n",
    "print(\"‚úÖ Visualization methods added to EDA Assistant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis and Advanced Visualizations\n",
    "\n",
    "def correlation_analysis(data, numeric_columns):\n",
    "    \"\"\"Perform correlation analysis on numeric columns\"\"\"\n",
    "    if len(numeric_columns) < 2:\n",
    "        print(\"Need at least 2 numeric columns for correlation analysis.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîó CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = data[numeric_columns].corr()\n",
    "    \n",
    "    # Display high correlations\n",
    "    high_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                high_corr.append({\n",
    "                    'Feature 1': corr_matrix.columns[i],\n",
    "                    'Feature 2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    if high_corr:\n",
    "        print(\"‚ö° HIGH CORRELATIONS (|r| > 0.7):\")\n",
    "        high_corr_df = pd.DataFrame(high_corr)\n",
    "        print(high_corr_df.sort_values('Correlation', key=abs, ascending=False))\n",
    "    else:\n",
    "        print(\"No high correlations found (|r| > 0.7)\")\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.2f', cbar_kws={\"shrink\": .8}, mask=mask)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "def outlier_detection(data, numeric_columns):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    print(\"üö® OUTLIER DETECTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    outlier_counts = {}\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "        outlier_counts[col] = len(outliers)\n",
    "    \n",
    "    outlier_df = pd.DataFrame(list(outlier_counts.items()), \n",
    "                             columns=['Column', 'Outlier Count'])\n",
    "    outlier_df['Outlier %'] = (outlier_df['Outlier Count'] / len(data)) * 100\n",
    "    outlier_df = outlier_df.sort_values('Outlier Count', ascending=False)\n",
    "    \n",
    "    print(outlier_df)\n",
    "    \n",
    "    # Plot outliers\n",
    "    if len(numeric_columns) > 0:\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        data[numeric_columns].boxplot(ax=plt.gca())\n",
    "        plt.title('Box Plots for Outlier Detection')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return outlier_df\n",
    "\n",
    "# Add advanced analysis methods\n",
    "def add_advanced_methods():\n",
    "    EDAAssistant.correlation_analysis = lambda self: correlation_analysis(self.data, self.numeric_columns)\n",
    "    EDAAssistant.outlier_detection = lambda self: outlier_detection(self.data, self.numeric_columns)\n",
    "\n",
    "add_advanced_methods()\n",
    "print(\"‚úÖ Advanced analysis methods added to EDA Assistant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated EDA with ydata-profiling (Pandas Profiling)\n",
    "\n",
    "def automated_eda_report(data, title=\"Automated EDA Report\"):\n",
    "    \"\"\"Generate automated EDA report using ydata-profiling\"\"\"\n",
    "    try:\n",
    "        from ydata_profiling import ProfileReport\n",
    "        \n",
    "        print(\"ü§ñ Generating Automated EDA Report...\")\n",
    "        print(\"This may take a few minutes for large datasets...\")\n",
    "        \n",
    "        # Create profile report\n",
    "        profile = ProfileReport(\n",
    "            data, \n",
    "            title=title,\n",
    "            explorative=True,\n",
    "            minimal=False\n",
    "        )\n",
    "        \n",
    "        # Display in notebook\n",
    "        profile.to_notebook_iframe()\n",
    "        \n",
    "        # Save HTML report (optional)\n",
    "        # profile.to_file(\"eda_report.html\")\n",
    "        \n",
    "        return profile\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå ydata-profiling not installed. Install with: pip install ydata-profiling\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating automated report: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def sweetviz_analysis(data, target_column=None):\n",
    "    \"\"\"Generate EDA report using Sweetviz\"\"\"\n",
    "    try:\n",
    "        import sweetviz as sv\n",
    "        \n",
    "        print(\"üç≠ Generating Sweetviz EDA Report...\")\n",
    "        \n",
    "        # Create report\n",
    "        if target_column and target_column in data.columns:\n",
    "            report = sv.analyze(data, target_feat=target_column)\n",
    "        else:\n",
    "            report = sv.analyze(data)\n",
    "        \n",
    "        # Show report\n",
    "        report.show_notebook()\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå sweetviz not installed. Install with: pip install sweetviz\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating Sweetviz report: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Add automated EDA methods\n",
    "def add_automated_methods():\n",
    "    EDAAssistant.automated_eda_report = lambda self, title=\"Automated EDA Report\": automated_eda_report(self.data, title)\n",
    "    EDAAssistant.sweetviz_analysis = lambda self, target_column=None: sweetviz_analysis(self.data, target_column)\n",
    "\n",
    "add_automated_methods()\n",
    "print(\"‚úÖ Automated EDA methods added to EDA Assistant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594baa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering and Analysis\n",
    "\n",
    "def feature_engineering_insights(data, numeric_columns, categorical_columns):\n",
    "    \"\"\"Provide insights for feature engineering\"\"\"\n",
    "    print(\"üîß FEATURE ENGINEERING INSIGHTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # Check for skewed numeric features\n",
    "    print(\"üìä SKEWNESS ANALYSIS:\")\n",
    "    skewness = data[numeric_columns].skew().abs().sort_values(ascending=False)\n",
    "    highly_skewed = skewness[skewness > 1]\n",
    "    if len(highly_skewed) > 0:\n",
    "        print(f\"Highly skewed features (|skew| > 1): {list(highly_skewed.index)}\")\n",
    "        insights.append(\"Consider log transformation for highly skewed features\")\n",
    "    else:\n",
    "        print(\"No highly skewed features found\")\n",
    "    \n",
    "    # Check for high cardinality categorical features\n",
    "    print(\"\\nüéØ CARDINALITY ANALYSIS:\")\n",
    "    high_cardinality = []\n",
    "    for col in categorical_columns:\n",
    "        unique_ratio = data[col].nunique() / len(data)\n",
    "        if unique_ratio > 0.5:\n",
    "            high_cardinality.append((col, data[col].nunique(), unique_ratio))\n",
    "    \n",
    "    if high_cardinality:\n",
    "        print(\"High cardinality categorical features:\")\n",
    "        for col, nunique, ratio in high_cardinality:\n",
    "            print(f\"  {col}: {nunique} unique values ({ratio:.2%} of total)\")\n",
    "        insights.append(\"Consider grouping rare categories or using embedding techniques\")\n",
    "    else:\n",
    "        print(\"No high cardinality categorical features found\")\n",
    "    \n",
    "    # Check for potential date features\n",
    "    print(\"\\nüìÖ POTENTIAL DATE FEATURES:\")\n",
    "    date_candidates = []\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype == 'object':\n",
    "            sample_values = data[col].dropna().head(10).astype(str)\n",
    "            if any(len(str(val)) in [8, 10, 19] and any(c in str(val) for c in ['-', '/', ':']) for val in sample_values):\n",
    "                date_candidates.append(col)\n",
    "    \n",
    "    if date_candidates:\n",
    "        print(f\"Potential date columns: {date_candidates}\")\n",
    "        insights.append(\"Consider converting date strings to datetime and extracting features\")\n",
    "    else:\n",
    "        print(\"No obvious date columns found\")\n",
    "    \n",
    "    # Feature interaction suggestions\n",
    "    print(\"\\nüîó FEATURE INTERACTION SUGGESTIONS:\")\n",
    "    if len(numeric_columns) >= 2:\n",
    "        insights.append(\"Consider creating ratio features between numeric columns\")\n",
    "        insights.append(\"Consider polynomial features for important numeric features\")\n",
    "    \n",
    "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"{i}. {insight}\")\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def data_quality_assessment(data):\n",
    "    \"\"\"Assess data quality issues\"\"\"\n",
    "    print(\"üîç DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    quality_issues = []\n",
    "    \n",
    "    # Duplicate rows\n",
    "    duplicates = data.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"‚ö†Ô∏è  Duplicate rows: {duplicates} ({duplicates/len(data)*100:.2f}%)\")\n",
    "        quality_issues.append(f\"Remove {duplicates} duplicate rows\")\n",
    "    else:\n",
    "        print(\"‚úÖ No duplicate rows found\")\n",
    "    \n",
    "    # Constant columns\n",
    "    constant_cols = [col for col in data.columns if data[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        print(f\"‚ö†Ô∏è  Constant columns: {constant_cols}\")\n",
    "        quality_issues.append(f\"Consider removing constant columns: {constant_cols}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No constant columns found\")\n",
    "    \n",
    "    # High missing value columns\n",
    "    missing_threshold = 0.5\n",
    "    high_missing = data.isnull().mean()\n",
    "    high_missing_cols = high_missing[high_missing > missing_threshold].index.tolist()\n",
    "    if high_missing_cols:\n",
    "        print(f\"‚ö†Ô∏è  High missing value columns (>{missing_threshold*100}%): {high_missing_cols}\")\n",
    "        quality_issues.append(f\"Consider dropping or imputing high missing columns\")\n",
    "    else:\n",
    "        print(f\"‚úÖ No columns with >{missing_threshold*100}% missing values\")\n",
    "    \n",
    "    # Mixed data types in object columns\n",
    "    print(\"\\nüîÑ MIXED DATA TYPE CHECK:\")\n",
    "    for col in data.select_dtypes(include=['object']).columns:\n",
    "        sample = data[col].dropna().head(100)\n",
    "        numeric_count = sum(pd.to_numeric(sample, errors='coerce').notna())\n",
    "        if 0 < numeric_count < len(sample):\n",
    "            print(f\"‚ö†Ô∏è  {col}: Mixed numeric/text data\")\n",
    "            quality_issues.append(f\"Review mixed data types in column: {col}\")\n",
    "    \n",
    "    print(f\"\\nüìù Quality Issues Found: {len(quality_issues)}\")\n",
    "    for i, issue in enumerate(quality_issues, 1):\n",
    "        print(f\"{i}. {issue}\")\n",
    "    \n",
    "    return quality_issues\n",
    "\n",
    "# Add feature engineering methods\n",
    "def add_feature_methods():\n",
    "    EDAAssistant.feature_engineering_insights = lambda self: feature_engineering_insights(self.data, self.numeric_columns, self.categorical_columns)\n",
    "    EDAAssistant.data_quality_assessment = lambda self: data_quality_assessment(self.data)\n",
    "\n",
    "add_feature_methods()\n",
    "print(\"‚úÖ Feature engineering methods added to EDA Assistant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Analysis (if applicable)\n",
    "\n",
    "def time_series_analysis(data, datetime_columns):\n",
    "    \"\"\"Analyze time series data if available\"\"\"\n",
    "    if not datetime_columns:\n",
    "        print(\"No datetime columns found for time series analysis.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìÖ TIME SERIES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for col in datetime_columns:\n",
    "        print(f\"\\nüïí Analyzing {col}:\")\n",
    "        \n",
    "        # Basic info\n",
    "        date_range = data[col].max() - data[col].min()\n",
    "        print(f\"   Date range: {data[col].min()} to {data[col].max()}\")\n",
    "        print(f\"   Duration: {date_range}\")\n",
    "        \n",
    "        # Create time-based features\n",
    "        data[f'{col}_year'] = data[col].dt.year\n",
    "        data[f'{col}_month'] = data[col].dt.month\n",
    "        data[f'{col}_day'] = data[col].dt.day\n",
    "        data[f'{col}_dayofweek'] = data[col].dt.dayofweek\n",
    "        data[f'{col}_hour'] = data[col].dt.hour if hasattr(data[col].dt, 'hour') else None\n",
    "        \n",
    "        # Plot time series\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Count by date\n",
    "        date_counts = data[col].dt.date.value_counts().sort_index()\n",
    "        plt.subplot(2, 2, 1)\n",
    "        date_counts.plot(kind='line')\n",
    "        plt.title(f'Records Count Over Time - {col}')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Monthly pattern\n",
    "        monthly_counts = data[f'{col}_month'].value_counts().sort_index()\n",
    "        plt.subplot(2, 2, 2)\n",
    "        monthly_counts.plot(kind='bar', color='lightgreen')\n",
    "        plt.title(f'Records by Month - {col}')\n",
    "        plt.xlabel('Month')\n",
    "        \n",
    "        # Day of week pattern\n",
    "        dow_counts = data[f'{col}_dayofweek'].value_counts().sort_index()\n",
    "        plt.subplot(2, 2, 3)\n",
    "        dow_counts.plot(kind='bar', color='orange')\n",
    "        plt.title(f'Records by Day of Week - {col}')\n",
    "        plt.xlabel('Day of Week (0=Monday)')\n",
    "        \n",
    "        # Hourly pattern (if applicable)\n",
    "        if f'{col}_hour' in data.columns and data[f'{col}_hour'].notna().any():\n",
    "            hourly_counts = data[f'{col}_hour'].value_counts().sort_index()\n",
    "            plt.subplot(2, 2, 4)\n",
    "            hourly_counts.plot(kind='bar', color='red')\n",
    "            plt.title(f'Records by Hour - {col}')\n",
    "            plt.xlabel('Hour')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Add time series methods\n",
    "def add_time_series_methods():\n",
    "    EDAAssistant.time_series_analysis = lambda self: time_series_analysis(self.data, self.datetime_columns)\n",
    "\n",
    "add_time_series_methods()\n",
    "print(\"‚úÖ Time series analysis methods added to EDA Assistant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ced812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete EDA Workflow\n",
    "\n",
    "def complete_eda_analysis(eda_assistant, target_column=None):\n",
    "    \"\"\"\n",
    "    Run complete EDA analysis workflow\n",
    "    \n",
    "    Parameters:\n",
    "    eda_assistant: EDAAssistant instance with loaded data\n",
    "    target_column: Target column for supervised learning (optional)\n",
    "    \"\"\"\n",
    "    if eda_assistant.data is None:\n",
    "        print(\"‚ùå No data loaded. Please load data first using eda.load_data()\")\n",
    "        return\n",
    "    \n",
    "    print(\"üöÄ STARTING COMPLETE EDA ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. Basic Information\n",
    "    print(\"\\n\" + \"=\"*20 + \" STEP 1: BASIC INFORMATION \" + \"=\"*20)\n",
    "    eda_assistant.basic_info()\n",
    "    \n",
    "    # 2. Statistical Summary\n",
    "    print(\"\\n\" + \"=\"*20 + \" STEP 2: STATISTICAL SUMMARY \" + \"=\"*20)\n",
    "    eda_assistant.statistical_summary()\n",
    "    \n",
    "    # 3. Missing Values Visualization\n",
    "    print(\"\\n\" + \"=\"*20 + \" STEP 3: MISSING VALUES \" + \"=\"*20)\n",
    "    eda_assistant.plot_missing_values()\n",
    "    \n",
    "    # 4. Distribution Analysis\n",
    "    print(\"\\n\" + \"=\"*20 + \" STEP 4: DISTRIBUTION ANALYSIS \" + \"=\"*20)\n",
    "    eda_assistant.plot_numeric_distributions()\n",
    "    eda_assistant.plot_categorical_distributions()\n",
    "    \n",
    "    # 5. Correlation Analysis\n",
    "    print(\"\\n\" + \"=\"*20 + \" STEP 5: CORRELATION ANALYSIS \" + \"=\"*20)\n",
    "    eda_assistant.correlation_analysis()\n",
    "    \n",
    "    # 6. Outlier Detection\n",
    "    print(\"\\n\" + \"=\"*20 + \" STEP 6: OUTLIER DETECTION \" + \"=\"*20)\n",
    "    eda_assistant.outlier_detection()\n",
    "    \n",
    "    # 7. Feature Engineering Insights\n",
    "    print(\"\\n\" + \"=\"*20 + \" STEP 7: FEATURE ENGINEERING \" + \"=\"*20)\n",
    "    eda_assistant.feature_engineering_insights()\n",
    "    \n",
    "    # 8. Data Quality Assessment\n",
    "    print(\"\\n\" + \"=\"*20 + \" STEP 8: DATA QUALITY \" + \"=\"*20)\n",
    "    eda_assistant.data_quality_assessment()\n",
    "    \n",
    "    # 9. Time Series Analysis (if applicable)\n",
    "    print(\"\\n\" + \"=\"*20 + \" STEP 9: TIME SERIES ANALYSIS \" + \"=\"*20)\n",
    "    eda_assistant.time_series_analysis()\n",
    "    \n",
    "    # 10. Target Variable Analysis (if provided)\n",
    "    if target_column and target_column in eda_assistant.data.columns:\n",
    "        print(\"\\n\" + \"=\"*20 + f\" STEP 10: TARGET ANALYSIS ({target_column}) \" + \"=\"*20)\n",
    "        target_analysis(eda_assistant.data, target_column, eda_assistant.numeric_columns, eda_assistant.categorical_columns)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ COMPLETE EDA ANALYSIS FINISHED!\")\n",
    "    print(\"üí° Consider running automated EDA for additional insights:\")\n",
    "    print(\"   eda.automated_eda_report()\")\n",
    "    print(\"   eda.sweetviz_analysis()\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "def target_analysis(data, target_column, numeric_columns, categorical_columns):\n",
    "    \"\"\"Analyze relationship between features and target variable\"\"\"\n",
    "    print(f\"üéØ TARGET VARIABLE: {target_column}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    target_type = 'numeric' if target_column in numeric_columns else 'categorical'\n",
    "    print(f\"Target type: {target_type}\")\n",
    "    print(f\"Target distribution:\")\n",
    "    if target_type == 'categorical':\n",
    "        print(data[target_column].value_counts())\n",
    "    else:\n",
    "        print(data[target_column].describe())\n",
    "    \n",
    "    # Visualize target\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    if target_type == 'categorical':\n",
    "        plt.subplot(1, 2, 1)\n",
    "        data[target_column].value_counts().plot(kind='bar', color='lightblue')\n",
    "        plt.title(f'Distribution of {target_column}')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        data[target_column].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "        plt.title(f'Proportion of {target_column}')\n",
    "    else:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        data[target_column].hist(bins=30, alpha=0.7, color='lightblue')\n",
    "        plt.title(f'Distribution of {target_column}')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        data[target_column].plot(kind='box', color='lightblue')\n",
    "        plt.title(f'Box plot of {target_column}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature-target relationships\n",
    "    print(f\"\\nüîó FEATURE-TARGET RELATIONSHIPS:\")\n",
    "    \n",
    "    # Numeric features vs target\n",
    "    numeric_features = [col for col in numeric_columns if col != target_column]\n",
    "    if numeric_features and target_type == 'numeric':\n",
    "        # Correlation with target\n",
    "        correlations = data[numeric_features + [target_column]].corr()[target_column].drop(target_column)\n",
    "        correlations = correlations.abs().sort_values(ascending=False)\n",
    "        print(\"Top correlations with target:\")\n",
    "        print(correlations.head(10))\n",
    "        \n",
    "        # Plot top correlations\n",
    "        top_features = correlations.head(4).index\n",
    "        if len(top_features) > 0:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "            axes = axes.flatten()\n",
    "            for i, feature in enumerate(top_features):\n",
    "                if i < 4:\n",
    "                    data.plot.scatter(x=feature, y=target_column, ax=axes[i], alpha=0.6)\n",
    "                    axes[i].set_title(f'{feature} vs {target_column}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Add complete workflow methods\n",
    "def add_workflow_methods():\n",
    "    EDAAssistant.complete_eda_analysis = lambda self, target_column=None: complete_eda_analysis(self, target_column)\n",
    "\n",
    "add_workflow_methods()\n",
    "print(\"‚úÖ Complete EDA workflow methods added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data Generation and Usage Example\n",
    "\n",
    "def generate_sample_data(n_samples=1000):\n",
    "    \"\"\"Generate sample data for testing the EDA assistant\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate sample data\n",
    "    data = {\n",
    "        'age': np.random.normal(35, 12, n_samples).astype(int),\n",
    "        'income': np.random.lognormal(10, 0.8, n_samples),\n",
    "        'score': np.random.beta(2, 5, n_samples) * 100,\n",
    "        'category': np.random.choice(['A', 'B', 'C', 'D'], n_samples, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "        'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples),\n",
    "        'is_premium': np.random.choice([True, False], n_samples, p=[0.3, 0.7]),\n",
    "        'registration_date': pd.date_range('2020-01-01', '2023-12-31', periods=n_samples),\n",
    "        'rating': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.05, 0.1, 0.3, 0.35, 0.2])\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some missing values\n",
    "    missing_indices = np.random.choice(df.index, size=int(0.1 * n_samples), replace=False)\n",
    "    df.loc[missing_indices, 'income'] = np.nan\n",
    "    \n",
    "    missing_indices = np.random.choice(df.index, size=int(0.05 * n_samples), replace=False)\n",
    "    df.loc[missing_indices, 'score'] = np.nan\n",
    "    \n",
    "    # Add some outliers\n",
    "    outlier_indices = np.random.choice(df.index, size=int(0.02 * n_samples), replace=False)\n",
    "    df.loc[outlier_indices, 'age'] = np.random.choice([120, 150, 200], size=len(outlier_indices))\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Sample data generation function created!\")\n",
    "print(\"\\nüìñ USAGE INSTRUCTIONS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Load your data:\")\n",
    "print(\"   eda = EDAAssistant()\")\n",
    "print(\"   eda.load_data('your_dataset.csv')\")\n",
    "print(\"\\n2. Or generate sample data for testing:\")\n",
    "print(\"   sample_data = generate_sample_data()\")\n",
    "print(\"   eda.data = sample_data\")\n",
    "print(\"   eda._analyze_column_types()\")\n",
    "print(\"\\n3. Run complete EDA analysis:\")\n",
    "print(\"   eda.complete_eda_analysis(target_column='rating')\")\n",
    "print(\"\\n4. Or run individual analyses:\")\n",
    "print(\"   eda.basic_info()\")\n",
    "print(\"   eda.plot_missing_values()\")\n",
    "print(\"   eda.correlation_analysis()\")\n",
    "print(\"   eda.outlier_detection()\")\n",
    "print(\"\\n5. Generate automated reports:\")\n",
    "print(\"   eda.automated_eda_report()\")\n",
    "print(\"   eda.sweetviz_analysis(target_column='rating')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beaa659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ DEMONSTRATION: Run this cell to see the EDA Assistant in action!\n",
    "\n",
    "# Generate sample data\n",
    "print(\"üé≤ Generating sample data...\")\n",
    "sample_data = generate_sample_data(1000)\n",
    "\n",
    "# Initialize EDA Assistant with sample data\n",
    "eda.data = sample_data\n",
    "eda._analyze_column_types()\n",
    "\n",
    "print(f\"\\n‚úÖ Sample dataset created with {len(sample_data)} rows and {len(sample_data.columns)} columns\")\n",
    "print(f\"Columns: {list(sample_data.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüëÄ First 5 rows of sample data:\")\n",
    "print(sample_data.head())\n",
    "\n",
    "print(\"\\nüéØ Now you can run any EDA analysis:\")\n",
    "print(\"‚Ä¢ eda.complete_eda_analysis(target_column='rating') - Full analysis\")\n",
    "print(\"‚Ä¢ eda.basic_info() - Basic information\")\n",
    "print(\"‚Ä¢ eda.correlation_analysis() - Correlation analysis\") \n",
    "print(\"‚Ä¢ eda.plot_missing_values() - Missing values visualization\")\n",
    "print(\"‚Ä¢ eda.outlier_detection() - Outlier detection\")\n",
    "print(\"‚Ä¢ eda.automated_eda_report() - Automated pandas profiling report\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ EDA LLM Assistant is ready to use!\")\n",
    "print(\"Try running: eda.complete_eda_analysis(target_column='rating')\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö EDA Assistant Documentation and Additional Features\n",
    "\n",
    "print(\"üìö EDA LLM ASSISTANT - COMPREHENSIVE DOCUMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüéØ MAIN FEATURES:\")\n",
    "print(\"1. Automated Data Loading - Supports CSV, Excel, JSON, Parquet, SQLite\")\n",
    "print(\"2. SQLite Database Support - Table exploration, custom queries, JOINs\")\n",
    "print(\"3. Basic Data Exploration - Shape, types, missing values, unique counts\")\n",
    "print(\"4. Statistical Analysis - Descriptive statistics, skewness, kurtosis\")\n",
    "print(\"5. Visualization Suite - Distributions, correlations, missing values\")\n",
    "print(\"6. Correlation Analysis - Heatmaps and high correlation detection\")\n",
    "print(\"7. Outlier Detection - IQR method with box plots\")\n",
    "print(\"8. Feature Engineering Insights - Skewness, cardinality, date detection\")\n",
    "print(\"9. Data Quality Assessment - Duplicates, constants, mixed types\")\n",
    "print(\"10. Time Series Analysis - Temporal patterns and feature extraction\")\n",
    "print(\"11. Automated EDA Reports - Pandas Profiling and Sweetviz integration\")\n",
    "\n",
    "print(\"\\nüîß AVAILABLE METHODS:\")\n",
    "methods = [\n",
    "    \"load_data(file_path, table_name, query) - Load data from file or SQLite\",\n",
    "    \"explore_sqlite_database(file_path) - Explore SQLite database structure\",\n",
    "    \"basic_info() - Display basic dataset information\",\n",
    "    \"statistical_summary() - Show statistical summary\",\n",
    "    \"plot_missing_values() - Visualize missing values\",\n",
    "    \"plot_numeric_distributions() - Plot numeric distributions\",\n",
    "    \"plot_categorical_distributions() - Plot categorical distributions\",\n",
    "    \"correlation_analysis() - Analyze correlations\",\n",
    "    \"outlier_detection() - Detect outliers\",\n",
    "    \"feature_engineering_insights() - Get feature engineering tips\",\n",
    "    \"data_quality_assessment() - Assess data quality\",\n",
    "    \"time_series_analysis() - Analyze time series data\",\n",
    "    \"automated_eda_report() - Generate pandas profiling report\",\n",
    "    \"sweetviz_analysis() - Generate Sweetviz report\",\n",
    "    \"complete_eda_analysis() - Run full EDA workflow\"\n",
    "]\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"‚Ä¢ {method}\")\n",
    "\n",
    "print(\"\\nüí° QUICK START EXAMPLES:\")\n",
    "print(\"```python\")\n",
    "print(\"# Initialize\")\n",
    "print(\"eda = EDAAssistant()\")\n",
    "print(\"\")\n",
    "print(\"# Load CSV/Excel/JSON/Parquet\")\n",
    "print(\"eda.load_data('your_data.csv')\")\n",
    "print(\"\")\n",
    "print(\"# Load SQLite database\")\n",
    "print(\"eda.explore_sqlite_database('database.db')  # Explore structure first\")\n",
    "print(\"eda.load_data('database.db', table_name='customers')  # Load specific table\")\n",
    "print(\"eda.load_data('database.db', query='SELECT * FROM customers WHERE age > 30')\")\n",
    "print(\"\")\n",
    "print(\"# Run complete analysis\")\n",
    "print(\"eda.complete_eda_analysis(target_column='your_target')\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\nüé® CUSTOMIZATION OPTIONS:\")\n",
    "print(\"‚Ä¢ Modify visualization parameters (colors, figure sizes)\")\n",
    "print(\"‚Ä¢ Adjust thresholds for outlier detection\")\n",
    "print(\"‚Ä¢ Customize automated report settings\")\n",
    "print(\"‚Ä¢ Add custom analysis functions\")\n",
    "\n",
    "print(\"\\nüìä SUPPORTED FILE FORMATS:\")\n",
    "print(\"‚Ä¢ CSV (.csv)\")\n",
    "print(\"‚Ä¢ Excel (.xlsx, .xls)\")\n",
    "print(\"‚Ä¢ JSON (.json)\")\n",
    "print(\"‚Ä¢ Parquet (.parquet)\")\n",
    "print(\"‚Ä¢ SQLite (.db, .sqlite, .sqlite3)\")\n",
    "print(\"\")\n",
    "print(\"üìà SUPPORTED DATA TYPES:\")\n",
    "print(\"‚Ä¢ Numeric: int, float, complex\")\n",
    "print(\"‚Ä¢ Categorical: object, category\")\n",
    "print(\"‚Ä¢ Temporal: datetime64\")\n",
    "print(\"‚Ä¢ Boolean: bool\")\n",
    "\n",
    "print(\"\\nüöÄ PERFORMANCE TIPS:\")\n",
    "print(\"‚Ä¢ For large datasets (>100K rows), consider sampling first\")\n",
    "print(\"‚Ä¢ Use automated reports sparingly on very wide datasets\")\n",
    "print(\"‚Ä¢ Filter columns of interest before analysis\")\n",
    "print(\"‚Ä¢ Save reports to HTML for sharing\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Happy Exploring! Your data insights await!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
